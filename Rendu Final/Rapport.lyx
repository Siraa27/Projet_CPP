#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extreport
\begin_preamble
\usepackage{minted}
\usepackage[breaklinks,colorlinks=true,linkcolor=black,
citecolor=red, urlcolor=blue]{hyperref}

\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{fncychap}
\usepackage{lettrine}

\definecolor{gris}{cmyk}{0.7,0.6,0.5,0.3}
\definecolor{bleu}{cmyk}{1,0.9,0.1,0}

\newcommand{\insertrefproj}[1]{}
\newcommand{\refproj}[1]{\renewcommand{\insertrefproj}{\textbf{\color{gris}#1}}}

\fancyhead[R]{\color{gris}\thepage}
\fancyfoot[L]{\insertrefproj}
\fancyfoot[R]{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.2pt}

\title{}
\refproj{}

\pagestyle{plain}
\end_preamble
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language french
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Implémentation d'un réseau de neurones
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Author
Amina El Bachari, Israa Ben Sassi, Camille Goujet, 
\begin_inset Newline newline
\end_inset

Mehdi Helal et Rafael Quilbier
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

à l'attention de Jean-Philippe Kotowicz
\end_layout

\begin_layout Standard
\SpecialChar softhyphen

\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
Depuis une dizaine d'années, les réseaux de neurones sont au cœur de l’intellige
nce artificielle et sont désormais un enjeu majeur dans de nombreux secteurs
 tels que la santé, l'énergie, les industries....
\begin_inset Newline newline
\end_inset

Avec l'amélioration de la puissance de calcul des machines et des résultats
 spectaculaires obtenus avec le deep learning, 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Pour toutes ces raisons, vous trouverez dans le présent rapport, une description
 détaillée d'un réseau de neurones, l'explication de chacun des modules
 qui le composent et finalement, l'implémentation de notre propre réseau
 de neurones que nous .
 PAS FINI 
\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Section
Bibliographie
\end_layout

\begin_layout Subsection
Définitions importantes 
\end_layout

\begin_layout Subsubsection
Neurone dit formel ou artificiel 
\end_layout

\begin_layout Standard
Le neurone formel est l'unité élémentaire des réseaux de neurones artificiels
 dans lesquels il est associé à ses semblables pour calculer des fonctions
 arbitrairement complexes, utilisées pour diverses applications en intelligence
 artificielle.
 Mathématiquement, le neurone formel est une fonction à plusieurs variables
 et à valeurs réelles.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Neurone.png
	scale 45

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Schématisation algorithmique d’un neurone artificiel
\end_layout

\end_inset


\end_layout

\end_inset

Cette représentation est appelée le perceptron, un algorithme d’apprentissage
 supervisé pour les classifications binaires linéaires.
 Cela fait beaucoup de mots inconnus alors arrêtons nous un instant sur
 chacun de ces termes car ils seront importants pour la suite ! 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Algorithme : le perceptron est une suite d’opérations et de calcul = la
 somme des entrées, leur pondération, la vérification d’une condition et
 la production d’un résultat d’activation.
 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Apprentissage : l’algorithme doit être “entraîné”, c’est à dire qu’en fonction
 d’une prédiction voulue, le poids des différentes entrées va évoluer et
 il faudra trouver une valeur optimale pour chacune.
 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Supervisé : l’algorithme trouve les valeurs optimales de ses poids à partir
 d’une base de données d’exemples dont on connaît déjà la prédiction.
 Par exemple on a une base de données de photos de banane et on “règle”
 notre algorithme jusqu’à ce que chaque photo (ou presque) soit classé comme
 banane.
 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Classification : l’algorithme permet de prédire une caractéristique en sortie
 et cette caractéristique sert à classer les différentes entrées entre elles.
 Par exemple, trouver toutes les bananes dans un panel de photos de fruits.
 
\end_layout

\begin_layout Subsubsection
Réseau de neurones 
\end_layout

\begin_layout Standard
Un réseau de neurones est en général composé d'une succession de couches
 dont chacune prend ses entrées sur les sorties de la précédente.
 Chaque couche (i) est composée de Ni neurones, prenant leurs entrées sur
 les Ni-1 neurones de la couche précédente.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename ReseauNeurones.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration d'un réseau de neurones simple
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Couche 
\end_layout

\begin_layout Standard
En effet, un réseau de neurones est composé d’une couche d’entrées ( “inputs
 layer”), d’une couche de sortie (“outputs layers”) et d’au moins une couche
 cachée (“hidden layer”) qui fait le lien entre entrée et sortie.
 Toutes ces couches sont composés de plusieurs neurones qui sont eux-mêmes
 reliés les uns aux autres par des poids.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Couches.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration des différentes couches
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Fonction d’activation 
\end_layout

\begin_layout Standard
La fonction d’activation (ou fonction de seuillage, ou encore fonction de
 transfert) sert à introduire une non-linéarité dans le fonctionnement du
 neurone.
 Les fonctions de seuillage présentent généralement trois intervalles :
 en dessous du seuil, le neurone est non-actif (souvent dans ce cas, sa
 sortie vaut 0 ou -1) ; aux alentours du seuil, une phase de transition
 ; au-dessus du seuil, le neurone est actif (souvent dans ce cas, sa sortie
 vaut 1).
 Des exemples classiques de fonctions d’activation sont : 
\end_layout

\begin_layout Standard
- La fonction sigmoïde.
 
\end_layout

\begin_layout Standard
- La fonction tangente hyperbolique.
 
\end_layout

\begin_layout Standard
- La fonction de Heaviside.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename fonctions-dactivation.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Différentes fonctions d'activation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Fonctionnement d'un réseau de neurones
\end_layout

\begin_layout Standard
Pour comprendre son fonctionnement, étudions un modèle mathématique simple
 d'un neurone biologique : le modèle de McCulloch et Pitts (1943).
 
\begin_inset Newline newline
\end_inset

Considérons 
\begin_inset Formula $n$
\end_inset

 entrées 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\mathbb{R}$
\end_inset

.
 Un neurone fonctionne en 2 phases.
 Tout d'abord, il effectue la somme pondérée des entrées :
\begin_inset Formula 
\[
I=\sum_{i=1}^{n}\omega_{i}x_{i}
\]

\end_inset

avec 
\begin_inset Formula $\omega_{i}\in\mathbb{R}$
\end_inset

 le poids de la 
\begin_inset Formula $i^{ème}$
\end_inset

 entrée.
 
\begin_inset Newline newline
\end_inset

Ensuite, la fonction d'activation 
\begin_inset Formula $f$
\end_inset

 vérifie si la valeur calculée est supérieure au seuil requis et détermine
 si le neurone est actif ou non.
 Pour ce faire, on compare 
\begin_inset Formula $I$
\end_inset

 à un seuil 
\begin_inset Formula $T$
\end_inset

 : si 
\begin_inset Formula $I\geq T$
\end_inset

 alors le neurone est actif et transmet le signal, sinon il est inactif.
 Dans le modèle de McCulloch et Pitts, on a :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(I)=\begin{cases}
1 & si\,I\geq T\\
-1 & sinon
\end{cases}
\]

\end_inset

Notons que lorsque les neurones sont reliés uniquement par des connexions
 directes (i.e vers un neurone de la couche suivante), l’activation des différent
es couches est réalisée de manière synchrone : la sortie de tous les neurones
 de la 
\begin_inset Formula $1^{ère}$
\end_inset

 couche est calculée, puis celle de la 
\begin_inset Formula $2^{e}$
\end_inset

...
 Dans le cas d’un réseau possédant des connexions latérales (de haut en
 bas vers un neurone d'une même couche ou bien vers un neurone d'une couche
 précédente), l’ordre de mise à jour des sorties des différents neurones
 est important, car chaque nouvelle sortie va pouvoir modifier le calcul
 de la sortie d’un autre neurone.
 On peut mettre à jour des sorties de manière asynchrone et aléatoire (i.e
 les neurones sont choisis aléatoirement) ou bien de manière synchrone (toutes
 les sorties sont mises à jour en même temps).
\end_layout

\begin_layout Standard
Au fur et à mesure des itérations, les poids sont modifiés par apprentissage.
 Il les calcule en fonction des couples entrée/sortie désirées (on parle
 d'apprentissage supervisé) ou indépendamment d'une sortie désirée (on parle
 d'auto-organisation) par des petites adaptations successives.
 
\begin_inset Newline newline
\end_inset

Soit 
\begin_inset Formula $\omega_{ij}$
\end_inset

 le poids de la connexion entre les neurones 
\begin_inset Formula $i$
\end_inset

 et 
\begin_inset Formula $j$
\end_inset

 à un instant donné discret 
\begin_inset Formula $t$
\end_inset

.
 Après une itération d'apprentissage, la nouvelle valeur est 
\begin_inset Formula $\omega_{ij}(t+1)=\omega_{ij}(t)+\Delta\omega_{ij}$
\end_inset

.
 
\begin_inset Newline newline
\end_inset

Il existe plusieurs règles d'apprentissage courantes comme la règle de Hebb,
 la règle d'apprentissage compétitif, etc que nous détaillerons dans notre
 projet lorsque nous les utiliserons.
 
\end_layout

\begin_layout Subsection
Architecture des réseaux de neurones
\end_layout

\begin_layout Standard
On appelle architecture d'un réseau de neurones sa forme.
 On distingue 4 types de réseaux de neurones :
\end_layout

\begin_layout Standard
- les réseaux de neurones Feed-forwarded
\end_layout

\begin_layout Standard
- les réseaux de neurones récurrents (RNN)
\end_layout

\begin_layout Standard
- les réseaux de neurones à résonance
\end_layout

\begin_layout Standard
- les réseaux de neurones auto-organisés.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Le choix de telle ou telle architecture est une question essentielle lors
 de la construction d'un réseau.
 Chacune possède ses forces et ses faiblesses.
 Ainsi, en fonction de l'application que l'on souhaite faire de notre algorithme
, on se portera sur une architecture en particulier.
 
\begin_inset Newline newline
\end_inset

On peut aussi souligner que souvent, afin d'obtenir un meilleur résultat,
 plusieurs types d'architectures sont combinées.
 Nous avons préalablement cité 4 grandes familles d'architectures, mais
 il existe des dizaines de réseaux particuliers pour chacune d'elles.
 Voyons plus en détails ces 4 architectures :
\end_layout

\begin_layout Subsubsection
Réseaux de neurones feed forwarded
\end_layout

\begin_layout Standard
L'appellation Feed-Forward signifie que l'information traverse le réseau
 de neurones de l'entrée à la sortie sans retour en arrière.
 En français, il est nommé : 
\begin_inset Quotes fld
\end_inset

réseau de neurones à propagation avant
\begin_inset Quotes frd
\end_inset

.
 Dans ce type de réseaux, on distingue les réseaux monocouches et les réseaux
 multicouches.
 
\begin_inset Newline newline
\end_inset

Dans les réseaux monocouches, le perceptron est dit 
\begin_inset Quotes fld
\end_inset

simple
\begin_inset Quotes frd
\end_inset

, car il est constitué que d'une couche d'entrée et une couche de sortie.
 
\begin_inset Newline newline
\end_inset

À l'inverse, dans les réseaux multicouches, les perceptrons sont qualifiés
 de multicouches, car ils disposent de plusieurs couches cachées entre l'entrée
 et la sortie.
\end_layout

\begin_layout Standard
Ces 2 types de réseaux de neurones ont des intérêts différents : le réseau
 multicouche est plus adapté pour traiter des informations non-linéaires.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename ResForward.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Réseau Feed-Forwarded
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Réseaux de neurones récurrents
\end_layout

\begin_layout Standard
Plus complexe et moins intuitif, les réseaux de neurones récurrents traitent
 les informations de manière circulaire.
 Un réseau est qualifié de récurrent si sa structure possède au moins un
 cycle.
 Contrairement l'architecture de type Feed Forwarded, dans les réseaux récurrent
s, l'information circule dans les deux sens.
 Ils peuvent posséder une couche ou plusieurs.
 L'intérêt est de conserver de l'information en mémoire et de la laisser
 accessible à tout instant ultérieur.
 C'est pourquoi les réseaux de neurones récurrents sont particulièrement
 bien adaptés aux applications faisant intervenir un contexte, comme la
 reconnaissance de forme.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Elman_ReseauRecurrent.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Réseau récurrent de type Elman
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Réseaux de neurones à résonance
\end_layout

\begin_layout Standard
L'appellation de ce type de réseaux fait référence à son fonctionnement.
 Lorsqu'un neurone est activé, son activation est renvoyée à tous les autres
 neurones du réseau.
 Cela provoque des oscillations, d'où l'emploi du terme 
\begin_inset Quotes fld
\end_inset

résonance
\begin_inset Quotes frd
\end_inset

.
 
\begin_inset Newline newline
\end_inset

Pour mieux comprendre l'objectif de cette architecture, étudions le modèle
 ART (Adaptative Resonance Theory) conçu par Gail Carpenter et Stephen Grossberg.
 Il existe de nombreux modèles ART, (ART1, ART2, fuzzy ART etc.) qui utilisent
 l'apprentissage supervisé, ou non supervisé.
 L'objectif général des modèles ART est de résoudre le dilemme entre stabilité
 et plasticité.
 
\begin_inset Newline newline
\end_inset

Selon un article du laboratoire d'Analyse Cognitive de l'information à Montréal
 : «la plasticité rend compte de la capacité du réseau à s'adapter aux informati
ons nouvelles, et la stabilisation mesure la capacité du réseau à organiser
 les informations connues en ensembles stables.».
 
\begin_inset Newline newline
\end_inset

Finalement, le principe des modèles ART est d'apprendre de manière autonome,
 à s'adapter, et à se stabiliser en même temps.
 Ainsi, ces modèles sont capables de choisir entre une information pertinente
 à prendre en compte, et une information superflue, qui pourrait donner
 lieu à un surapprentissage.
\end_layout

\begin_layout Subsubsection
Réseaux de neurones auto-organisés
\end_layout

\begin_layout Standard
Ce type de réseau de neurones est surtout utilisé dans le traitement d'informati
ons spéciales.
 En effet grâce à des méthodes d'apprentissage non supervisé, ce type de
 réseau est capable de répartir en différentes classes de grands espaces
 de données.
\end_layout

\begin_layout Section
Objectifs et fonctionnalités 
\end_layout

\begin_layout Standard
Tout au long de ce projet, nos objectifs seront les suivants :
\end_layout

\begin_layout Subsection
Création d’un réseau de neurones 
\end_layout

\begin_layout Subsection
Application d'un réseau de neurones à un exemple de la liste suivante :
 
\end_layout

\begin_layout Subsubsection
Les classifications de données, texte ou image 
\end_layout

\begin_layout Standard
L'objectif est d'élaborer un système capable d'affecter à une donnée, une
 image ou à un texte non-structuré, un tag qui correspond à une classe bien
 précise.
 
\begin_inset Newline newline
\end_inset

On cite deux exemples d’utilisation très importants.
 
\begin_inset Newline newline
\end_inset

D’une part, la classification de documents imprimés est une tâche cruciale
 dans de nombreuses chaînes de traitement; par exemple, l’automatisation
 de tâches bureautiques afin de classifier les documents imprimés selon
 des catégories telles que : lettres, publicités, plans et cartes, articles
 de presse, etc… 
\begin_inset Newline newline
\end_inset

Il faut tout d’abord extraire les données textuelles ou les images utilisées
 pour ensuite les classifier.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

D’autre part, la reconnaissance des émotions, très utilisée sur les réseaux
 sociaux.
 Qu’il s’agisse de la détection de comportement violent à travers un texte
 ou une image (DeepBreath) ou de l'analyse de l’impact émotionnel sur des
 compagnes de marketing, etc… 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Expression faciale.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration avec les visages
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Classifieur.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration avec le classifieur
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Prédiction de données
\end_layout

\begin_layout Standard
Enfin les réseaux de neurones sont très utilisés pour mettre au point des
 modèles de prédictions à partir d'échantillons de données.
 La prédiction d'indicateurs financiers est un exemple très parlant.
 En effet « en 2011, 40 % des ordres donnés sur le CAC40 sont totalement
 automatisés à l’aide d’algorithmes informatiques ».
 Aujourd'hui, cet outil fournit d'excellents résultats dans le domaine de
 détection des entreprises en future difficulté.
 
\begin_inset Newline newline
\end_inset

Les réseaux de neurones peuvent aussi prédire des phénomènes qui nous semblent
 plus aléatoires et non linéaires tout en prenant en compte d'éventuelles
 imprécisions des données fournies en entrée.
 Une équipe de scientifiques s'est intéressée à la prévision des crues du
 bassin-versant de l’Eure à la station de Louviers en Normandie grâce à
 des réseaux de neurones.
 Le modèle pluie-débit en résultant a permis de prendre en compte l'imprécision
 des données fournies en entrée et ainsi d'établir des prévisions fiables
 en quelques secondes sur les grandes crues à venir dans les 48 heures.
 Cet exemple illustre parfaitement l'importance que peuvent avoir les réseaux
 de neurones dans la prédiction de phénomène non-linéaires.
\end_layout

\begin_layout Section
Manuel d'utilisation préliminaire
\end_layout

\begin_layout Itemize
Choisir une application : 
\end_layout

\begin_deeper
\begin_layout Itemize
Classification
\end_layout

\begin_layout Itemize
Prédiction
\end_layout

\end_deeper
\begin_layout Itemize
Choisir un type de réseau :
\end_layout

\begin_deeper
\begin_layout Itemize
Réseau Feed Forwarded
\end_layout

\begin_layout Itemize
Réseau Récurrent
\end_layout

\end_deeper
\begin_layout Itemize
Saisir les caractéristiques du réseau : 
\end_layout

\begin_deeper
\begin_layout Itemize
Importation des données à partir d'un fichier.
\end_layout

\begin_layout Itemize
Saisie manuelle
\end_layout

\begin_deeper
\begin_layout Itemize
Saisir le nombre de couches cachées
\end_layout

\begin_layout Itemize
Saisir le nombre de neurones par couches
\end_layout

\begin_layout Itemize
Choix des poids initiaux 
\end_layout

\begin_deeper
\begin_layout Itemize
Remplissage de manière aléatoire
\end_layout

\begin_layout Itemize
Remplissage à la main ( une valeur pour tous les poids)
\end_layout

\end_deeper
\begin_layout Itemize
Choix des biais ( nbNeuroneparCoucheCachee*nbCoucheCachee+nbNeuroneCoucheSortie)
\end_layout

\begin_deeper
\begin_layout Itemize
Remplissage de manière aléatoire
\end_layout

\begin_layout Itemize
Remplissage à la main ( une valeur pour tous les poids)
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Donner le nom de fichier de données
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename diagramme des choix.png
	lyxscale 50
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset

En pratique, voici ce que nous obtenons : 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Capture d’écran 2021-05-23 à 22.56.50.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Manuel d'utilisation manuelle
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Capture d’écran 2021-05-23 à 23.12.51.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Manuelle d'utilisation Fichier
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Spécification
\end_layout

\begin_layout Standard
Maintenant que nous nous sommes familiariser avec toutes les notions que
 peuvent avoir leur importance dans la compréhension d'un réseau de neurones,
 nous allons à présent étudier les côtés plus techniques de ces objets afin
 de mener à bien l'implémentation de notre propre réseau.
\end_layout

\begin_layout Section
Conception préliminaire
\end_layout

\begin_layout Subsection
Diagramme de cas d’utilisation
\end_layout

\begin_layout Standard
Voici notre diagramme de cas d'utilisation final : 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename DiagrammeCasUtilisation.png
	lyxscale 50
	scale 28
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Diagramme de cas d'utilisation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Descriptions des cas d’utilisation
\end_layout

\begin_layout Itemize
Fournir les données d’entrées : pour faire ceci, on construit un tableau
 de la dimension adéquate correspondant à celle des données que l’on donne
 au réseau.
 Si nos données sont sous forme de fichier ou autre, il faut les convertir
 dans le langage utilisé avant de pouvoir les utiliser.
 Ce travail sera fait dans la classe Entrées.
 
\end_layout

\begin_layout Itemize
Choix des fonctionnalités, des paramètres et du type de réseau : notre programme
 propose les différentes options à l'utilisateur de manière intéractive
 pour lancer la partie du programme correspondant au choix de l’utilisateur.
 
\end_layout

\begin_layout Subsection
Diagrammes de séquence 
\end_layout

\begin_layout Standard
Afin de visualiser les interactions entre les composants, nous avons réalisé
 plusieurs diagrammes de séquence pour chaque phase.
\begin_inset Newline newline
\end_inset

Nous commençons avec un diagramme de séquence assez général :
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename General.png
	lyxscale 30
	scale 55
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Diagramme de séquence
\end_layout

\end_inset


\end_layout

\end_inset

 Ensuite, nous avons détaillé les différentes phases du premier diagramme.
 Notamment, pour la création d'un réseau : 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename CreationReseau.png
	lyxscale 30
	scale 55
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Diagramme de séquence
\end_layout

\end_inset


\end_layout

\end_inset

 ainsi que pour l'initialisation d'un réseau prédéfini dans un fichier 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename ChoisiPredef.png
	lyxscale 30
	scale 55
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Diagramme de séquence
\end_layout

\end_inset


\end_layout

\end_inset

.
 Finalement, nous avons réalisé un diagramme de séquence lorsqu'un utilisateur
 formule sa requête : 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename FormulerRequete.png
	lyxscale 30
	scale 55
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Diagramme de séquence
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Le choix des données 
\end_layout

\begin_layout Standard
Pour le moment, nous allons se concentrer sur une classification supervisée
 des données.
 Voyons cela sur l’exemple des iris de Fisher.
 
\begin_inset Newline newline
\end_inset

Les Iris de Fisher correspondent à 150 fleurs décrites par 4 variables quantitat
ives : 
\end_layout

\begin_layout Itemize
longueur du sépale 
\end_layout

\begin_layout Itemize
largeur du sépale 
\end_layout

\begin_layout Itemize
longueur du pétale 
\end_layout

\begin_layout Itemize
largeur du pétale 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Les 150 fleurs sont réparties en 3 différentes espèces : 
\end_layout

\begin_layout Itemize
iris setosa 
\end_layout

\begin_layout Itemize
iris versicolor 
\end_layout

\begin_layout Itemize
iris virginica.
 
\end_layout

\begin_layout Standard
Le fichier va être sous la forme suivante : 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Fichier données iris.png
	lyxscale 30
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
exemple du fichier
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
On peut donc voir nos données (entrée du programme) comme : 
\end_layout

\begin_layout Itemize
Pour une fleur (entrée du réseau) : un vecteur de dimension 4 représentant
 les 4 variables explicatives d'une fleur
\begin_inset Formula 
\[
f=(x_{1},x_{2},x_{3},x_{4})^{T}
\]

\end_inset


\end_layout

\begin_layout Itemize
Pour les classe (la sortie du réseau): un vecteur de dimension 3 représentant
 les probabilités des 3 potentielles classes d'appartenance (setosa, versicolor,
 virginica)
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula 
\[
  c=(y_{1},y_{2},y_{3})^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Il faut donc lire les mesures, les mettre en entrée du réseau, calculer
 le résultat en propageant l’information de l’entrée vers la sortie couche
 par couche, prendre la sortie maximale parmi les neurones de sortie et
 afficher le résultat.
\begin_inset Newline newline
\end_inset

Donc notre réseau, cela ressemblera à quelque chose comme ça : 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename reseau neuronne Fisher.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Réseau de neurones appliqué aux iris de Fisher
\end_layout

\end_inset


\end_layout

\end_inset

Se pose le problème suivant : comment à partir des mesures réalisées sur
 une fleur inconnue trouver le type d’iris à l'aide d’un réseau de neurones
 ? 
\end_layout

\begin_layout Subsection
Tests d’intégration
\end_layout

\begin_layout Standard
Ici, on explicite tous les différents scénarios possibles engageant un réseau
 de neurones.
 Cela nous permettra de n’oublier aucune fonctionnalité dans notre code
 et dans nos diagrammes.
 Ces scénarios seront ensuite repris à la fin pour effectuer différents
 tests, et ainsi vérifier le bon fonctionnement de notre programme.
 
\end_layout

\begin_layout Subsubsection
Scénarios de création d’un réseau : 
\end_layout

\begin_layout Paragraph
Réseau vide :
\end_layout

\begin_layout Standard
Si dans le choix des paramètres, on choisit 0 neurone et donc 0 couche,
 les utilisations que l’on pourra faire de ce réseau seront très limitées
 pour ne pas dire inexistantes.
 Pas de couche d'entrée, ni d’arrivée.
 Il serait utile de prévoir une exception pour éviter ce cas de figure/de
 potentielles erreurs.
\end_layout

\begin_layout Paragraph
Perceptron simple : 
\end_layout

\begin_layout Standard
On entre en machine : 2 pour le nombre de couches, autant de neurones que
 l’on veut pour la couche d’entrée et un seul neurone sur la couche de sortie.
 Le perceptron simple est un modèle de prédiction (supervisé) linéaire.
 Ce schéma simple a un effet plus éducatif qu’autre chose, il sert de modèle
 pour le perceptron multicouches qui permet de régler des problèmes plus
 complexes.
\end_layout

\begin_layout Paragraph
Perceptron multicouche : 
\end_layout

\begin_layout Standard
L’utilisateur entre en machine un nombre de couches supérieures à 2.
 Notons tout de même que choisir au dela de 6 à 10 couches entraine très
 souvent des problèmes d’overfitting, aussi appelée surapprentissage (le
 réseau est incapable se généraliser, car il est trop adapté aux données
 d’appretissage).
 Il serait peut être bon de définir un nombre de couches par défaut pour
 orienter les utilisateurs novices.
 Puis l’utilisateur saisit de manière itérative le nombre de neurones pour
 chaque couche y compris la couche de sortie qui peut comporter plusieurs
 neurones.
 Ensuite les neurones des couches i seront reliés à la couche i+1 suivant
 le type de réseau sélectionné par l’utilisateur.
 
\end_layout

\begin_layout Subsubsection
Scénarios relatifs aux fonctions d’activation : 
\end_layout

\begin_layout Standard
Ici, il est très difficile de délimiter notre projet puisqu’il n’existe
 pas de règles propres au choix de la fonction d’activation utilisée pour
 telle couche, tel réseau ou telle fonctionnalité de ce réseau.
 
\begin_inset Newline newline
\end_inset

On peut tout de même émettre quelques pistes : 
\end_layout

\begin_layout Itemize
Dans le cas d'un problème de 
\emph on
régression
\emph default
, il n'est pas nécessaire de transformer la somme pondérée reçue en entrée.
 La fonction d'activation est la fonction identité, elle retourne ce qu'elle
 a reçu en entier.
 (pour un perceptron simple)
\end_layout

\begin_layout Itemize
Dans le cas d'un problème de 
\emph on
classification binaire
\emph default
, on peut utiliser une fonction de seuil : 
\begin_inset Formula 
\[
s\left(w_{0}+\stackrel[j=1]{p}{\sum}w_{j}x_{j}\right)=\begin{cases}
0 & si\,\left(w_{0}+\stackrel[j=1]{p}{\sum}w_{j}x_{j}\right)<0\\
1 & sinon
\end{cases}
\]

\end_inset

Comme dans le cas de la 
\emph on
régression logistique
\emph default
, on peut aussi utiliser une fonction 
\series bold
sigmoïde
\series default
 pour prédire la 
\begin_inset Formula $probabilité$
\end_inset

 d'appartenir à la classe positive.
 
\end_layout

\begin_layout Itemize
Dans le cas d'un problème de 
\emph on
classification multi-classe,
\emph default
 nous allons modifier l'architecture du perceptron.
 Au lieu d'utiliser une seule unité de sortie, il va en utiliser autant
 que de classes.
 Chacune de ces unités sera connectée à toutes les unités d'entrée.
 On aura donc ainsi 
\begin_inset Formula $K(p+1)$
\end_inset

 poids de connexion, où 
\begin_inset Formula $K$
\end_inset

 est le nombre de classes.
 On peut alors utiliser comme fonction d'activation la fonction 
\series bold
softmax
\series default
.
 Il s'agit d'une généralisation de la sigmoïde.
\begin_inset Newline newline
\end_inset

Si la sortie pour la classe 
\begin_inset Formula $k$
\end_inset

 est suffisamment plus grande que celles des autres classes, son activation
 sera proche de
\begin_inset Formula $1$
\end_inset

 tandis que l'activation des autres sera proche de 
\begin_inset Formula $0.$
\end_inset

 On peut donc aussi considérer qu'il s'agit d'une version différentiable
 du maximum, ce qui nous aidera grandement pour l'apprentissage.
 
\end_layout

\begin_layout Subsubsection
Scénarios relatifs aux poids :
\end_layout

\begin_layout Standard
Pour entraîner un perceptron, c'est-à-dire apprendre les poids de connexion,
 nous allons chercher à minimiser l'erreur de prédiction sur le jeu d'entraîneme
nt.
 On peut initialiser les poids de différentes manières, soit on initialise
 les poids manuellement (souvent on prend la valeur 0,5 (valeur moyenne
 entre 0 et 1)) soit on initialise les poids en créant une fonction qui
 nous renverra un nombre aléatoire entre 0 et 1 pour chaque poids.
 
\begin_inset Newline newline
\end_inset

Puis pour les faire évoluer à chaque itération jusqu’à convergence de l’algorith
me, on implémente la méthode de descente du gradient.
 Grâce à celle-ci, le poids à l’étape 
\begin_inset Formula $(i)$
\end_inset

 devient à l’étape 
\begin_inset Formula $(i+1)$
\end_inset

 : 
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Courbe erreur poids.png
	scale 80
	rotateOrigin center

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Courbe de l'erreur en fonction du poids
\begin_inset Newline newline
\end_inset

où ɳ est la vitesse d’apprentissage.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Scénarios relatifs aux utilisations : 
\end_layout

\begin_layout Standard
On compte 4 cas d’utilisations pour un réseau de neurones.
 Nous nous sommes limités à proposer la prédiction de données et la classificati
on.
 Lorsque l’utilisateur saisit le type d'utilisation, un algorithme type
 va se lancer.
 Voyons chaque cas un par un en détail : 
\end_layout

\begin_layout Paragraph
Classification 
\end_layout

\begin_layout Standard
Il est recommandé à l’utilisateur d’utiliser un perceptron multi-couches.
 Puis deux modélisations lui sont proposées.
 
\end_layout

\begin_layout Itemize
Soit l’utilisateur saisit le nombre 
\begin_inset Formula $K$
\end_inset

 de classes qui correspond au nombre de sorties du réseau de neurones.
 Chaque neurone de sortie indique la probabilité d’appartenance à la 
\begin_inset Formula $k^{ème}$
\end_inset

 classe.
 La classe avec la probabilité la plus forte est la classe d’appartenance
 de la donnée fournie en entrée.
 
\end_layout

\begin_layout Itemize
Soit l’utilisateur choisit de modéliser un réseau pour chaque classe 
\begin_inset Formula $n=1..K$
\end_inset

.
 Dans ce cas, il y a donc un unique neurone de sortie pour chacun des réseaux
 indiquant la probabilité d’appartenance des données à la nième classe.
 Dans cette modélisation, une donnée passe par 
\begin_inset Formula $K$
\end_inset

 réseau de neurones.
 Puis la construction du réseau débute, avec comme nombre de neurones dans
 la couche d’entrée le cardinal des variables explicatives des données sélection
nées et le nombre de neurones de la couche de sortie égale à 
\begin_inset Formula $1$
\end_inset

 (pour 
\begin_inset Formula $K$
\end_inset

 réseaux).
 
\end_layout

\begin_layout Standard
Ensuite, l'utilisateur peut choisir un apprentissage supervisé ou non du
 réseau.
 S'il choisit un apprentissage supervisé il devra importer les données nécessair
e (toujours au bon format et organisé).
 Enfin, la dernière étape consiste à importer les données organisées au
 bon format que l’on souhaite classifier, puis à lancer l’algorithme.
 
\end_layout

\begin_layout Paragraph
Prédiction
\end_layout

\begin_layout Standard
Dans le cas d’une prédiction, nous considérons une unique modélisation.
 La couche d’entrée comporte 
\begin_inset Formula $n$
\end_inset

 neurones, égale à la cardinalité du nombre de variables explicatives, et
 la couche de sortie possède un unique neurone qui retourne la valeur prédite.
 L’utilisateur va donc saisir le nombre de variables explicatives que comporte
 son jeu de données.
 Puis la construction du réseau débute.
 Un fois construit, l'utilisateur spécifie s'il souhaite faire apprendre
 son réseau.
 Si oui, il rentre le fichier de données d’apprentissage.
 Une fois l’apprentissage effectué, il faut importer les valeurs des variables
 explicatives de la donnée que l’utilisateur souhaite prédire, puis lancer
 l’algorithme.
 
\end_layout

\begin_layout Section
Conception détaillée
\end_layout

\begin_layout Standard
Dans cette partie, nous expliquons en détail chaque module/classe composant
 notre code et les tests unitaires qui lui sont associés.
 
\begin_inset Newline newline
\end_inset

Cela permet d'avoir une vision précis, exhaustive et claire de l'implémentation
 produite.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Subsection
Diagramme de classe
\end_layout

\begin_layout Standard
Voici notre diagrammde de classe :
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename 189217488_195480449101442_4223601380712545224_n.png
	scale 22

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Diagramme de classe
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Réseau 
\end_layout

\begin_layout Subsubsection
La classe Réseau
\end_layout

\begin_layout Standard
Dans cette classe, on retrouve les méthodes qui permettent de manipuler
 un réseau de neurones, de sa création à sa destruction.
 Une première remarque importante sur ces codes : nous avons souvent été
 obligés de “dupliquer” certains codes car certaines couches cachées sont
 liées à la couche d’entrée ou bien à la couche de sortie, ce qui veut dire
 que les codes de la forme : 
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{couchecachee[i] = f(couchecachee[i-1], couchecachee[i+1])}
\end_layout

\end_inset

 ne fonctionne pas dans ce cas particulier.
 Par exemple, dans le constructeur de Réseau, nous avons traité le cas de
 la première couche cachée 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{couches[0]}
\end_layout

\end_inset

 à part.
 La fonction 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{erreur(int classeSolution)}
\end_layout

\end_inset

 calcule la norme au carré de la différence entre la sortie attendue (qui
 est donc fournie par l’utilisateur) et la sortie calculée par le réseau.
 Ensuite, la méthode 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{Apprentissage(string Donnees)}
\end_layout

\end_inset

 est une méthode qui modifie les poids et les biais du réseau à l’aide du
 fichier Donnees qui est un échantillon d’apprentissage.
 Elle construit l’échantillon d’apprentissage complet composé de 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{lesEntrees}
\end_layout

\end_inset

 et de 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{lesSorties}
\end_layout

\end_inset

 en faisant appel à la méthode 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{Remplissage(lesEntrees, lesSorties, Donnees)}
\end_layout

\end_inset

.
 Ensuite, elle divise cet échantillon en sous-échantillons appelés 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{EntreesSousEchan}
\end_layout

\end_inset

 et 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{SortiesSousEchan}
\end_layout

\end_inset

 puis elle appelle la méthode 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{BackPropagation(vector<CoucheEntrees*>& x, vector<CoucheSorties*
>& y)}
\end_layout

\end_inset

 sur chaque sous-échantillon.
 Cette dernière méthode va calculer le gradient de l’erreur moyen sur le
 sous-échantillon qui lui est donné en paramètres et modifier les poids
 et les biais en conséquences.
 C’est la stratégie de modification par lots ou batch.
 Pour être plus précis, la méthode 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{Backpropagation()}
\end_layout

\end_inset

 appelle la méthode 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{calcGradErr(CoucheSorties sAttendues)}
\end_layout

\end_inset

 qui permet de calculer le gradient de l’erreur sur un seul exemple sur
 chaque élément du sous-échantillon d’apprentissage, puis elle fait la moyenne
 des gradients obtenus et elle modifie les poids et les biais.
 Cette méthode 
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{calcGradErr(CoucheSorties sAttendues)}
\end_layout

\end_inset

 est la plus compliquée.
 Pour faire simple, elle utilise le principe de la rétropropagation du gradient
 : elle calcule en premier les derniers termes du gradient de l’erreur,
 c’est-à-dire les dérivées partielles de la fonction d’erreur par rapport
 aux poids et aux biais de la dernière couche du réseau (donc la couche
 de sortie).
 Une fois ces termes calculés, on en déduit la valeur des dérivées partielles
 par rapport aux poids et aux biais de la couche précédente, et ainsi de
 suite jusqu’à avoir calculé les dérivées partielles par rapport à tous
 les poids et tous les biais.
 À chaque étape, on stocke les dérivées partielles par rapport aux poids
 de la i-ème couche dans la variable 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{gradientErr[2*i]}
\end_layout

\end_inset

 et les dérivées partielles par rapport aux biais de la i-ème couche dans
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mintinline{c++}{gradientErr[2*i+1]}
\end_layout

\end_inset

.
 Enfin, chaque dérivée partielle est calculée en utilisant la formule suivante
 obtenue à l’aide de la chain rule : 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename chainRule.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Chain rule
\end_layout

\end_inset


\end_layout

\end_inset

avec 
\begin_inset Formula $C$
\end_inset

 qui désigne la fonction d’erreur, 
\begin_inset Formula $w_{ij}{}^{l}$
\end_inset

 qui est le poids reliant le i-ème neurone de la couche 
\begin_inset Formula $l$
\end_inset

 au 
\begin_inset Formula $j$
\end_inset

-ème neurone de la couche 
\begin_inset Formula $l-1$
\end_inset

, 
\begin_inset Formula $z_{i}^{l}$
\end_inset

 est le résultat de la pré-activation du 
\begin_inset Formula $i$
\end_inset

-ème neurone de la couche 
\begin_inset Formula $l$
\end_inset

 et 
\begin_inset Formula $a_{i}^{l}$
\end_inset

 est la fonction d’activation du 
\begin_inset Formula $i$
\end_inset

-ème neurone de la couche 
\begin_inset Formula $l$
\end_inset

.
 On cacule de la même manière les dérivées partielles par rapport aux biais
 en remplaçant les poids 
\begin_inset Formula $w_{ij}{}^{l}$
\end_inset

 par 
\begin_inset Formula $b_{i}^{l}$
\end_inset

 qui désigne le biais du 
\begin_inset Formula $i$
\end_inset

-ème neurone de la couche 
\begin_inset Formula $l$
\end_inset

.
\begin_inset Newline newline
\end_inset

Voir documentation doxygen.
\end_layout

\begin_layout Subsection
Couche
\end_layout

\begin_layout Standard
La fonction Couche a pour unique but de construire les objets Couche peu
 importe leur type (entrée, cachée, sortie).
 
\begin_inset Newline newline
\end_inset

Pour notre implémentation, nous avons choisi de coder les méthodes utiles
 dans les classes CoucheCachée et CoucheSorties uniquement puisqu'elles
 sont inutiles dans CoucheEntrées.
 
\begin_inset Newline newline
\end_inset

voir la documentation doxygen
\end_layout

\begin_layout Subsection
CoucheEntree
\end_layout

\begin_layout Subsubsection
La classe CoucheEntree
\end_layout

\begin_layout Standard
voir la documentation doxygen
\end_layout

\begin_layout Subsubsection
Les tests de la classe CoucheEntree
\end_layout

\begin_layout Paragraph
testConstructionSortie :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode:
\bar default
 void constructionSortie()
\series bold

\begin_inset Newline newline
\end_inset


\series default

\begin_inset Newline newline
\end_inset

Soit un fichier de données test qui contient les 4 valeurs utilisées dans
 l’exemple précédent.
 
\begin_inset Newline newline
\end_inset

il faut vérifier que la fonction constructionSorite remplit un tableau de
 neurone de dimension 4 comme suit : 
\begin_inset Newline newline
\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{4}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $6,3$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3,3$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $6,0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2,5$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Newline newline
\end_inset

Donc il faut créer un objet coucheEntrée qu’on initialise à l’aide d’un
 constructeur avec les valeurs du tableau et vérifier que le tableau de
 neurone de cet objet est égal au tableau de neurone de notre fonction test.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Paragraph
Resultat: 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename TestCoucheEntrees.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Resultat testConstructionSortie
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
CoucheSortie et CoucheCachee
\end_layout

\begin_layout Subsubsection
La classe CoucheSortie
\end_layout

\begin_layout Standard
voir la documentation doxygen.
\end_layout

\begin_layout Subsubsection
La classe CouchCachee
\end_layout

\begin_layout Standard
voir la documentation doxygen.
\begin_inset Newline newline
\end_inset

Comme vous allez le constater, nous avons fait le choix d'implémenter les
 fonctions preActivation() et activation() dans la classe CoucheCachée mais
 aussi dans CoucheSortie.
 Cela nous permet en fait de pouvoir modifier la fonction activation qui,
 bien souvent n'est pas la même selon si c'est une couche cachée ou de sortie.
\end_layout

\begin_layout Subsubsection
Les tests de la classe CoucheCachee et CoucheSortie
\end_layout

\begin_layout Paragraph
testPreactivation :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode: 
\bar default
double[] preActivation()
\series bold

\begin_inset Newline newline
\end_inset


\series default

\begin_inset Newline newline
\end_inset

Il faut tester si elle renvoie bien la somme des produits de la valeur du
 neurone avec le poids qui le relie au neurone étudié ajouté au biais de
 ce neurone.
 
\begin_inset Newline newline
\end_inset

Pour l'exemple des iris de Fisher :
\end_layout

\begin_layout Itemize
une couche d’entrée qui comprend 4 entrées x1, x2, x3 et x4 
\end_layout

\begin_layout Itemize
une couche cachée qui comprend 2 neurones n1 et n2
\end_layout

\begin_layout Standard
⇒ Il y a alors 8 poids et 2 biais 
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename TEST.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Exemple des iris de Fisher
\end_layout

\end_inset


\end_layout

\end_inset

La fonction de préactivation doit donnée comme valeur pour n1 : 
\begin_inset Newline newline
\end_inset


\begin_inset Formula $x1\times0,1+x2\times0,3+x3\times0,5+x4\times0,7+b1=0,63+0,99+3+1,75+0,5=\mathbf{{\color{purple}6,87}}$
\end_inset


\begin_inset Newline newline
\end_inset

Pour n2 : 
\begin_inset Formula ${\color{purple}\mathbf{8,98}}$
\end_inset


\begin_inset Newline newline
\end_inset

⇒ Donc on doit avoir 
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6,87
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8,98
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Paragraph
testActivation :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode:
\bar default
 double activation(double[])
\series bold

\begin_inset Newline newline
\end_inset


\series default
Nous allons considérer le cas de la fonction sigmoïde donnée par :
\begin_inset Formula 
\[
f(x)=\frac{1}{1+e^{x}}
\]

\end_inset

Il faut tester si elle renvoie la bonne valeur.
 
\begin_inset Newline newline
\end_inset

Pour cela, reprenons le résultat de la fonction de préactivation, on a alors
 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(6,87)=0.001
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Resultat:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename CoucheSorties.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename TestCoucheCachees.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Resultat Activation et testPreactivation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Interface:
\end_layout

\begin_layout Subsubsection
Classe Interface
\end_layout

\begin_layout Standard
voir la documentation doxygen.
\end_layout

\begin_layout Subsubsection
Les tests de la classe Interface
\end_layout

\begin_layout Paragraph
testLectureFichier :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode:
\bar default
 listeParamètres lectureFichier(String 'nomFich.csv')
\series bold

\begin_inset Newline newline
\end_inset


\series default
Nous allons d'abord tester la fonction lectureFichier().
 Pour cela, nous allons définir le jeu de données que nous allons utiliser
 ainsi que les résultats attendus.
 
\begin_inset Newline newline
\end_inset

Tout d'abord, faisons un point sur la manière de remplir un fichier des
 paramètres du futur réseau à construire :
\end_layout

\begin_layout Itemize

\emph on
Le premier paramètre
\series bold
\emph default
 
\series default
du fichier sera le type de réseau.
 Dans notre projet, seul le réseau forwarded sera fonctionel.
 Il sera représenté par l'entier 
\begin_inset Formula $1$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\emph on
Le second paramètre 
\emph default
du fichier est le cas d'utilisation du réseau (
\begin_inset Formula $1:$
\end_inset

classification, 
\begin_inset Formula $2:$
\end_inset

prédiction, 
\begin_inset Formula $3:$
\end_inset

identification des objets et 
\begin_inset Formula $4:$
\end_inset

 reconnaissance d'image).
 Rappelons que seule le choix 
\begin_inset Formula $1$
\end_inset

 et potentiellement 
\begin_inset Formula $2$
\end_inset

 seront implémentés.
 
\end_layout

\begin_layout Itemize

\emph on
Le troisième paramètre
\emph default
 compte le nombre de couches cachées, nbCouche.
 
\end_layout

\begin_layout Itemize
Le 
\emph on
quatrième paramètre
\emph default
 compte le nombre de neurones par couche cachée (classe CoucheCachée).
 
\end_layout

\begin_layout Itemize

\emph on
Le cinquième paramètre
\emph default
 du fichier est 0 ou 
\begin_inset Formula $x$
\end_inset

 un entier positif non nul :
\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

- 0 si l'initialisation de la matrice de poids initiaux est faite 
\emph on
aléatoirement
\emph default
.
 
\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

- 
\begin_inset Formula $x$
\end_inset

 si l'utilisateur choisit d'initaliser tous les poids à 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Enumerate

\series bold
Test avec un fichier vide:
\series default
 Entrer en paramétres un fichier en format .txt vide.
 
\begin_inset Newline newline
\end_inset

La méthode doit renvoyer un message d'erreur sur la console pour prévenir
 l'utilisateur, puis revenir sur le menu précédent.
 
\begin_inset Newline newline
\end_inset

Renvoie 
\begin_inset Formula $0$
\end_inset

 comme message d'erreur.
\end_layout

\begin_layout Enumerate

\series bold
Test avec un fichier au mauvais format:
\series default
 Entrer en paramètres un fichier en binaire, donc illisible par notre programme.
 
\begin_inset Newline newline
\end_inset

La méthode doit renvoyer un message d'erreur sur la console pour prévenir
 l'utilisateur, puis revenir sur le menu précédent (ainsi l'utilisateur
 pourra entrer manuellement les paramètres si son fichier n'est pas au bon
 format).
 
\begin_inset Newline newline
\end_inset

Renvoie 
\begin_inset Formula $-2$
\end_inset

 comme message d'erreur.
\end_layout

\begin_layout Enumerate

\series bold
Test avec un fichier mal rempli:
\series default
 Plusieurs tests sont envisageables.
 
\begin_inset Newline newline
\end_inset

Voyons en détails :
\end_layout

\begin_deeper
\begin_layout Standard
Fichier test 1: 
\begin_inset Formula $\left(2\text{ }1\text{ }3\text{ }2\text{ }5\text{ }2\text{ }0\right)$
\end_inset

 renvoie un message pour dire que le fichier est mal rempli (exemple : réseau
 non implémenté).
 
\begin_inset Newline newline
\end_inset

Retourne une liste avec la valeur 
\begin_inset Formula $-1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
Test avec un fichier bien écrit: 
\end_layout

\begin_deeper
\begin_layout Standard
Fichier test 1: 
\begin_inset Formula $\left(1\text{ }1\text{ }3\text{ }2\text{ }5\text{ }2\text{ }0\right)$
\end_inset

 renvoie une liste de paramètres=
\begin_inset Formula $\left(1\text{ }1\text{ }3\text{ }2\text{ }5\text{ }2\text{ }0\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Paragraph
testlectureParam() :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode:
\bar default
 listeParamètres lectureParam()
\series bold

\begin_inset Newline newline
\end_inset


\series default

\begin_inset Newline newline
\end_inset

Tout d'abord, faisons un point sur la manière dont l'utilisateur va saisir
 les paramètres, soit comment va fonctionner la fonction listeParametreslectureP
aram().
 
\begin_inset Newline newline
\end_inset

L'utilisateur entrera à la suite les différents paramètres dans l'ordre
 suivant (il sera guidé par le programme 
\begin_inset Formula $param1:$
\end_inset

 type de réseau, 
\begin_inset Formula $param2:$
\end_inset

 cas d'utilisation du réseau (rappel: 
\begin_inset Formula $1:$
\end_inset

classification, 
\begin_inset Formula $2:$
\end_inset

prédiction, 
\begin_inset Formula $3:$
\end_inset

identification des objets et 
\begin_inset Formula $4:$
\end_inset

 reconnaissance d'image), 
\begin_inset Formula $param3:$
\end_inset

 nombre de couches cachées, 
\begin_inset Formula $param4:$
\end_inset

 nombre de neurones par couche (une suite d'entiers séparés par des espaces),
 
\begin_inset Formula $param5:$
\end_inset

 choix de la matrice de poids initiaux.)
\end_layout

\begin_layout Enumerate
Test quand l'utilisateur entre de mauvais paramètres (par exemple entre
 des string au lieu d'entier etc..
 problème de type)
\end_layout

\begin_deeper
\begin_layout Enumerate
Test 1: saisie du premier paramètre: si 
\begin_inset Formula $param1=a$
\end_inset

 erreur de type, si 
\begin_inset Formula $param1=4$
\end_inset

 erreur réseau non implémenté, (propose une resaisie, ou quitter).
 
\end_layout

\begin_layout Enumerate
Test 2: saisie du second paramètre: avec 
\begin_inset Formula $param1=1$
\end_inset

 si 
\begin_inset Formula $param2=a$
\end_inset

 erreur de type, si 
\begin_inset Formula $param2=4$
\end_inset

 erreur cas d'utilisation non implémenté, (propose une resaisie, ou quitter).
 
\end_layout

\begin_layout Enumerate
Test 3: saisie du troisième paramètre: avec 
\begin_inset Formula $param1=1$
\end_inset

, 
\begin_inset Formula $param2=2$
\end_inset

, si 
\begin_inset Formula $param3=a$
\end_inset

 erreur de type.
\end_layout

\begin_layout Enumerate
Test 4: saisie du quatrième paramètre: avec 
\begin_inset Formula $param1=1$
\end_inset

, 
\begin_inset Formula $param2=2$
\end_inset

, 
\begin_inset Formula $param3=4$
\end_inset

, si 
\begin_inset Formula $param4=4\text{ }5\text{ }6\text{ }9\text{ }5\text{ }7$
\end_inset

, le programme ne sauvegarde que les 4 premier entiers.
\end_layout

\begin_layout Enumerate
Test 5: saisie du quatrième paramètre: avec 
\begin_inset Formula $param1=1$
\end_inset

, 
\begin_inset Formula $param2=2$
\end_inset

, 
\begin_inset Formula $param3=4$
\end_inset

, si 
\begin_inset Formula $param4=4\text{ }5$
\end_inset

, erreur trop couches sans neurones.
\end_layout

\begin_layout Enumerate
Test 6: saisie du quatrième paramètre: avec 
\begin_inset Formula $param1=1$
\end_inset

, 
\begin_inset Formula $param2=2$
\end_inset

, 
\begin_inset Formula $param3=4$
\end_inset

, 
\begin_inset Formula $param4=4\text{ }5\text{ }6\text{ }1$
\end_inset

, si 
\begin_inset Formula $param5=a$
\end_inset

 erreur de type.
\end_layout

\end_deeper
\begin_layout Enumerate
Test quand l'utilisateur entre de bons paramètres:
\end_layout

\begin_deeper
\begin_layout Enumerate
Test 1: saisie suivante: 
\begin_inset Formula $param1=1$
\end_inset

, 
\begin_inset Formula $param2=1$
\end_inset

, 
\begin_inset Formula $param3=4$
\end_inset

, 
\begin_inset Formula $param4=4\text{ }5\text{ }6\text{ }1$
\end_inset

, 
\begin_inset Formula $param5=0$
\end_inset

 renvoie la liste: 
\begin_inset Formula $\left(1\text{ }1\text{ }4\text{ }4\text{ }5\text{ }6\text{ }1\text{ }0\right)$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\end_deeper
\begin_layout Subparagraph*
Résultat:
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename Testinterfaceuti.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Resultat Test interface
\end_layout

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Subsection
Matrice
\end_layout

\begin_layout Subsubsection
La classe Matrice
\end_layout

\begin_layout Standard
voir la documentation doxygen
\end_layout

\begin_layout Subsubsection
Les tests de la classe Matrice
\end_layout

\begin_layout Standard
Les tests sur la classe Matrice sont de 2 types :
\end_layout

\begin_layout Enumerate
Fournit le bon résultat 
\end_layout

\begin_layout Enumerate
Traite le cas où les matrices ne sont pas de la même taille et vérifie si
 le nombre colonne est égal au nombre de lignes de la matrice passé en paramètre.
\end_layout

\begin_layout Paragraph
testProduit :
\end_layout

\begin_layout Standard

\bar under
prototype de la méthode:
\bar default
 testProduit(Matrice)
\end_layout

\begin_layout Enumerate
Test pour 2 matrices 3x3 quelconques et retourne le bon résultat
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename prodmatrice.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
1ère partie de test
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Test pour 2 matrices de taille différentes (1x2 et 2x3) et retourne le bon
 résultat (une matrice 1x3) 
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename prodmatrice2.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
2ème partie de test
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Test pour 2 matrices de tailles incompatibles pour un produit matriciel
 et retourner erreur.
 
\end_layout

\begin_layout Paragraph
Resultat: 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename TestMatrice.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Resultat TestMatrice
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
ResForwarded
\end_layout

\begin_layout Standard
voir la documentation doxygen
\end_layout

\begin_layout Subsection
Neurone
\end_layout

\begin_layout Standard
voir la documentation doxygen
\end_layout

\begin_layout Chapter
Conclusion
\end_layout

\begin_layout Standard
Ce projet nous a d'abord permis d'appliquer et d'appronfondir nos connaissances
 en 
\begin_inset Formula $C++$
\end_inset

 et en génie logiciel (
\emph on
Github, Agile...
\emph default
).
 Nous avons pris conscience de la nécessité de mettre en oeuvre des tests
 unitaires, nottament avec 
\emph on
cppunit
\emph default
, afin de vérifier au fur et à mesure le bon fonctionnement des différentes
 parties de notre code.
\begin_inset Newline newline
\end_inset

Attirés par la science des données, nous avons également pris plaisir à
 découvrir les réseaux de neurones et leur implémentation.
 Cependant, malgré un long travail, nous avons manqué de temps pour terminer
 notre projet.
 Un travail préliminaire important était nécessaire pour maitriser le sujet.
 
\begin_inset Newline newline
\end_inset

Enfin, nous avons su nous organiser afin de répartir la charge de travail
 et mettre à profit les qualités de chacun.
 
\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

http://benoit.decoux.free.fr/ENSEIGNEMENT/PROGRAMMATION/projet_RN_CPP.pdf 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

https://www.juripredis.com/fr/blog/id-19-demystifier-le-machine-learning-partie-2-
les-reseaux-de-neurones-artificiels
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

https://www.lebigdata.fr/perceptron-machine-learning
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

https://www.youtube.com/watch?v=sK9AbJ4P8ao
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"

\end_inset

https://zestedesavoir.com/forums/sujet/6567/classe-generique-pour-des-reseaux-de-
neurones-en-c/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

https://openclassrooms.com/fr/courses/4470406-utilisez-des-modeles-supervises-non
-lineaires/4730716-entrainez-un-reseau-de-neurones-simple
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

http://lexicometrica.univ-paris3.fr/jadt/jadt2000/pdf/47/47.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"

\end_inset

http://nicolascormier.com/documentation/ia/cours_IA/node102.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"

\end_inset

https://dataanalyticspost.com/Lexique/reseaux-de-neurones-recurrents/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"

\end_inset

https://halshs.archives-ouvertes.fr/halshs-02096266/document
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"

\end_inset

https://www.tandfonline.com/doi/pdf/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"

\end_inset

https://www.rncan.gc.ca/cartes-outils-publications/imagerie-satellitaire-photos-aer
/tutoriels-sur-la-teledetection/analyse-interpretation-dimages/classification-et
-analyse-des-images/9362
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13"

\end_inset

https://weave.eu/deep-learning-service-de-linformatique-affective/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"

\end_inset

https://ledatascientist.com/introduction-a-la-categorisation-de-textes/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-15"

\end_inset

https://www.aquiladata.fr/classification-dimages-et-detection-dobjets-par-cnn/
\end_layout

\end_body
\end_document
